{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import gzip\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\" #specify GPU\n",
        "import keras as K\n",
        "import tensorflow as tf\n",
        "from keras import backend\n",
        "from keras.backend.tensorflow_backend import set_session\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout"
      ],
      "metadata": {
        "id": "KEjPvUntru8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GaS1LzvlGUu"
      },
      "outputs": [],
      "source": [
        "def moving_average(a, n=3):\n",
        "    ret = np.cumsum(a, dtype=float)\n",
        "    ret[n:] = ret[n:] - ret[:-n]\n",
        "    return ret[n - 1:] / n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Berechnet einfach nur die Durchschnitte der jeweiligen Nachbarwerte eines Arrays. Dadurch wird es zwei Werte kürzer (also fast nichts) berücksichtigte Extremwerte aber wesentlich weniger."
      ],
      "metadata": {
        "id": "b3xZY-_XlPFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "exec(open(hyperparameter_file).read())"
      ],
      "metadata": {
        "id": "rZvcRmUPljQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wir laden die Hyperparamter (hier erstmal nur ein Dummy)"
      ],
      "metadata": {
        "id": "ZQEDVIX1lnhV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = gzip.open(data_file, 'rb')\n",
        "X_tr, X_val, X_train, X_test, y_tr, y_val, y_train, y_test = pickle.load(file)\n",
        "file.close()"
      ],
      "metadata": {
        "id": "vlkW9taQlrhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wir laden die gesplitteten Daten aus der normalisation Datei"
      ],
      "metadata": {
        "id": "slGnUrUHpYe6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = tf.ConfigProto(\n",
        "         allow_soft_placement=True,\n",
        "         gpu_options = tf.GPUOptions(allow_growth=True))\n",
        "set_session(tf.Session(config=config))"
      ],
      "metadata": {
        "id": "WPT58-YIpbyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Das hier sind Performance Parameter, die je nach System gut sein können (für uns aber wahrscheinlich nicht)."
      ],
      "metadata": {
        "id": "bMh5ANUxpfX3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "for i in range(len(layers)):\n",
        "    if i==0:\n",
        "        model.add(Dense(layers[i], input_shape=(X_tr.shape[1],), activation=act_func,\n",
        "                        kernel_initializer='he_normal'))\n",
        "        model.add(Dropout(float(input_dropout)))\n",
        "    elif i==len(layers)-1:\n",
        "        model.add(Dense(layers[i], activation='linear', kernel_initializer=\"he_normal\"))\n",
        "    else:\n",
        "        model.add(Dense(layers[i], activation=act_func, kernel_initializer=\"he_normal\"))\n",
        "        model.add(Dropout(float(dropout)))\n",
        "    model.compile(loss='mean_squared_error', optimizer=K.optimizers.SGD(lr=float(eta), momentum=0.5))"
      ],
      "metadata": {
        "id": "CW8SEplepoqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Das hier ist unser Modell, welches wir hier konfigurieren."
      ],
      "metadata": {
        "id": "DgAeU7Rrpr38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hist = model.fit(X_tr, y_tr, epochs=epochs, shuffle=True, batch_size=64, validation_data=(X_val, y_val))\n",
        "val_loss = hist.history['val_loss']\n",
        "model.reset_states()"
      ],
      "metadata": {
        "id": "LV6QU39Qp5iV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jetzt trainieren wir das Modell tatsächlich.\n",
        "\n",
        "ACHTUNG ! In unserem Code ist nicht das ganze Training drinnen. Die Hyperparameter wurden in mehreren Durchläufen mit verschiedenen Hyperparamtern ausgewahlt, der Code ist nur ein Bsp.\n",
        "\n",
        "Die Hyperparamter in der Datei sind dennoch korrekt."
      ],
      "metadata": {
        "id": "oHxUk-Oap8uL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "average_over = 15\n",
        "mov_av = moving_average(np.array(val_loss), average_over)\n",
        "smooth_val_loss = np.pad(mov_av, int(average_over/2), mode='edge')\n",
        "epo = np.argmin(smooth_val_loss)"
      ],
      "metadata": {
        "id": "pBZs4drgqkHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hier werden die Validation Losses geglättet und dann das Modell der Epoche mit dem geringsten Verlust gewählt.\n",
        "\n",
        "Das Avg wird über 15 Epochen gebildet und danach die Originallänge des Modells wiedererstellt."
      ],
      "metadata": {
        "id": "f50L19wqrqLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hist = model.fit(X_train, y_train, epochs=epo, shuffle=True, batch_size=64, validation_data=(X_test, y_test))\n",
        "test_loss = hist.history['val_loss']"
      ],
      "metadata": {
        "id": "6A6cxL8hr_l-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jetzt trainieren wir das echte Modell mit allen Daten (s. Tabelle welche Teile der Daten in normalisation.iypnb) bis zur errechneten Epoche."
      ],
      "metadata": {
        "id": "DF-ifmtAsSvd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(16,8))\n",
        "ax.plot(val_loss, label='validation loss')\n",
        "ax.plot(smooth_val_loss, label='smooth validation loss')\n",
        "ax.plot(test_loss, label='test loss')\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Z5v57l_csgNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jetzt plotten wir die Modellperformance!"
      ],
      "metadata": {
        "id": "hwU4XGkysrDf"
      }
    }
  ]
}
