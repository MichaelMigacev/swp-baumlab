{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**### Validation Research Michael Flanderka ###**\n",
        "\n",
        "Um die Validation implementierung zu verstehen müssen wir erst einmal die Datei \"normalization.iypnb\" verstehen.\n",
        "\n",
        "Als ersten Schritt erkläre ich diese:"
      ],
      "metadata": {
        "id": "0-rioslxZx7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import gzip\n",
        "\n",
        "# in this example tanh normalization is used\n",
        "# fold 0 is used for testing and fold 1 for validation (hyperparamter selection)\n",
        "norm = 'tanh'\n",
        "test_fold = 0\n",
        "val_fold = 1"
      ],
      "metadata": {
        "id": "HOEtcUSSaf1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hier wird die Normalisierungsstrategie gewählt. Wir nutzen tanh, also eine Begrenzung der Werte zwischen -1 und 1 (grob, nicht eine harte Grenze).\n",
        "\n",
        "Hierzu wird der Mittelwert entfernt und durch die Standardabweichung geteilt (Standardisierung) und anschließend wird die Hyperbolische Tangens-Funktion (tanh) angewendet.\n",
        "\n",
        "Die Daten werden in folgender Art und Weise genutzt:\n",
        "\n",
        "Fold        | Verwendung                    | Variable im Code     \n",
        "------------|-------------------------------|------------------------\n",
        "Fold 0      | Testset (nach dem Training)   | X_test, y_test         \n",
        "Fold 1      | Validierungsset für Tuning    | X_val, y_val           \n",
        "Folds 2–4   | Trainingsdaten                | X_tr, y_tr             \n",
        "Folds 1–4   | Trainingsdaten nach Tuning    | X_train, y_train       \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UhUvDqd7avsc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2 in the paper states:\n",
        "\n",
        "For data normalization we employed three different types of input normalization: (i) standardizing all inputs to zero mean and unit variance, (ii) standarizing and applying hyperbolic tangent and (iii) standardizing, hyperbolic tangent and standardizing again.\n",
        "\n",
        "This corresponds to the following code"
      ],
      "metadata": {
        "id": "iAZxkwp9iAyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(X, means1=None, std1=None, means2=None, std2=None, feat_filt=None, norm='tanh_norm'):\n",
        "    if std1 is None:                                                      #If std1 not given: calculate\n",
        "        std1 = np.nanstd(X, axis=0)\n",
        "    if feat_filt is None:                                                 #If standard deviation is 0 (non informative) throw data away\n",
        "        feat_filt = std1!=0\n",
        "    X = X[:,feat_filt]\n",
        "    X = np.ascontiguousarray(X)                                           #Data array needs to be continuous\n",
        "    if means1 is None:                                                    #Calc mean and standardize\n",
        "        means1 = np.mean(X, axis=0)\n",
        "    X = (X-means1)/std1[feat_filt]\n",
        "    if norm == 'norm':                                                    #Now we start the actual normalization (corresponds to the end of 2.2 in the paper):\n",
        "        return(X, means1, std1, feat_filt)\n",
        "    elif norm == 'tanh':\n",
        "        return(np.tanh(X), means1, std1, feat_filt)\n",
        "    elif norm == 'tanh_norm':\n",
        "        X = np.tanh(X)\n",
        "        if means2 is None:\n",
        "            means2 = np.mean(X, axis=0)\n",
        "        if std2 is None:\n",
        "            std2 = np.std(X, axis=0)\n",
        "        X = (X-means2)/std2\n",
        "        X[:,std2==0]=0\n",
        "        return(X, means1, std1, means2, std2, feat_filt)"
      ],
      "metadata": {
        "id": "8AhXP9z-gOH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parameter:**\n",
        "\n",
        "X: Eingabematrix (z. B. Merkmale von Proben)\n",
        "\n",
        "means1, std1: Mittelwert und Standardabweichung der Originaldaten (für erste Normalisierung)\n",
        "\n",
        "means2, std2: Mittelwert und Standardabweichung nach tanh (für zweite Normalisierung bei tanh_norm)\n",
        "\n",
        "feat_filt: Filtermaske, die Features mit Standardabweichung = 0 ausschließt\n",
        "\n",
        "norm: gewählte Normalisierungsstrategie – 'norm', 'tanh', oder 'tanh_norm'"
      ],
      "metadata": {
        "id": "KYI1zUxzgTtc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#contains the data in both feature ordering ways (drug A - drug B - cell line and drug B - drug A - cell line)\n",
        "#in the first half of the data the features are ordered (drug A - drug B - cell line)\n",
        "#in the second half of the data the features are ordered (drug B - drug A - cell line)\n",
        "file = gzip.open('X.p.gz', 'rb')\n",
        "X = pickle.load(file)\n",
        "file.close()"
      ],
      "metadata": {
        "id": "4MvNFPJNjKOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Q6iYGDF2jLR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#contains synergy values and fold split (numbers 0-4)\n",
        "labels = pd.read_csv('labels.csv', index_col=0)\n",
        "#labels are duplicated for the two different ways of ordering in the data\n",
        "labels = pd.concat([labels, labels])\n",
        "#In the end X contains both ordering ways, both labeled the same"
      ],
      "metadata": {
        "id": "aVwqsERxjSDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remember: test_fold = 0 and val_fold = 1 also see table above!!!\n",
        "#indices of training data for hyperparameter selection: fold 2, 3, 4\n",
        "idx_tr = np.where(np.logical_and(labels['fold']!=test_fold, labels['fold']!=val_fold))\n",
        "#indices of validation data for hyperparameter selection: fold 1\n",
        "idx_val = np.where(labels['fold']==val_fold)\n",
        "\n",
        "#indices of training data for model testing: fold 1, 2, 3, 4\n",
        "idx_train = np.where(labels['fold']!=test_fold)\n",
        "#indices of test data for model testing: fold 0\n",
        "idx_test = np.where(labels['fold']==test_fold)"
      ],
      "metadata": {
        "id": "M0LRGmPuj9VO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_tr    = X[idx_tr]\n",
        "X_val   = X[idx_val]\n",
        "X_train = X[idx_train]\n",
        "X_test  = X[idx_test]"
      ],
      "metadata": {
        "id": "96V6vYrqmHS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wir ordnen jetzt die Indixe zu tatsächlichen Daten zu"
      ],
      "metadata": {
        "id": "xbGfJ4HVjS_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_tr = labels.iloc[idx_tr]['synergy'].values\n",
        "y_val = labels.iloc[idx_val]['synergy'].values\n",
        "y_train = labels.iloc[idx_train]['synergy'].values\n",
        "y_test = labels.iloc[idx_test]['synergy'].values"
      ],
      "metadata": {
        "id": "LXpSnbqSmQA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Und ziehen uns die zugehörigen Zielwerte (Labels) heraus"
      ],
      "metadata": {
        "id": "fAIyVfEepg0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if norm == \"tanh_norm\":\n",
        "    X_tr, mean, std, mean2, std2, feat_filt = normalize(X_tr, norm=norm)\n",
        "    X_val, mean, std, mean2, std2, feat_filt = normalize(X_val, mean, std, mean2, std2,\n",
        "                                                          feat_filt=feat_filt, norm=norm)\n",
        "else:\n",
        "    X_tr, mean, std, feat_filt = normalize(X_tr, norm=norm)\n",
        "    X_val, mean, std, feat_filt = normalize(X_val, mean, std, feat_filt=feat_filt, norm=norm)"
      ],
      "metadata": {
        "id": "-UhVg3UcpmUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "X_val wird hier jeweils mit den Parametern aus dem Trainingsdurchlauf normalisiert, um Data Leakage zu vermeiden. Angenommen wir würden auch aus XVal Daten nehmen, um die normalisierung durchzuführen, würde das Modell bereits die Lösung für die Daten kennen und somit besser erscheinen, als es ist.\n",
        "\n",
        "Wichtig ist hier also, dass die Daten ERST getrennt und DANN die normalisierung berechnet wird. Ansonsten würden bereits statistische Informationen aus der Lösung ins Modell einfließen. Wichtig ist hierbei, dass wir die Parameter aus dem Trainingsdurchlauf weitergeben und NICHT eine erneute Normalisierung für den Validierungsdatensatz machen.\n",
        "\n",
        "Dieser Durchlauf ist für die **INNERE VALIDIERUNG, also die HYPERPARAMETERAUSWAHL!**"
      ],
      "metadata": {
        "id": "-oz2v4SKqHnB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if norm == \"tanh_norm\":\n",
        "    X_train, mean, std, mean2, std2, feat_filt = normalize(X_train, norm=norm)\n",
        "    X_test, mean, std, mean2, std2, feat_filt = normalize(X_test, mean, std, mean2, std2,\n",
        "                                                          feat_filt=feat_filt, norm=norm)\n",
        "else:\n",
        "    X_train, mean, std, feat_filt = normalize(X_train, norm=norm)\n",
        "    X_test, mean, std, feat_filt = normalize(X_test, mean, std, feat_filt=feat_filt, norm=norm)"
      ],
      "metadata": {
        "id": "GdKdvnSasd5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Das hier ist für den Outer Validiation Loop, also die eigentliche Modelloptimierung mit den gefundenen Hyperparametern."
      ],
      "metadata": {
        "id": "whUz9VivtRk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump((X_tr, X_val, X_train, X_test, y_tr, y_val, y_train, y_test),\n",
        "            open('data_test_fold%d_%s.p'%(test_fold, norm), 'wb'))"
      ],
      "metadata": {
        "id": "ePGQVIrIhc-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Als letztes speichern wir einfach die gesplitteten Daten, um diese exakt so wieder aufrufen zu können."
      ],
      "metadata": {
        "id": "ZdgUhGDXhlcP"
      }
    }
  ]
}