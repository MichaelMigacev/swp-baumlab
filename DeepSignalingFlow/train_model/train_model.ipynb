{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0db8eb83",
   "metadata": {},
   "source": [
    "### Training the DeepSignalingFlow model\n",
    "\n",
    "Code copied from the authors of \"Using DeepSignalingFlow to mine signaling ows interpreting mechanism of synergy of cocktails\".\n",
    "\n",
    "You can find the original code [here](https://github.com/FuhaiLiAiLab/DeepSignalingFlow/blob/main/geo_tmain_webgnn.py).\n",
    "\n",
    "All changes are marked with the comment `# Change in original code` with the following explanation of the changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b009115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â Imports\n",
    "\n",
    "import os\n",
    "import pdb\n",
    "import torch\n",
    "import argparse\n",
    "import tensorboardX\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from scipy import sparse\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import utils\n",
    "from geo_loader.read_geograph import read_batch\n",
    "from geo_loader.geograph_sampler import GeoGraphLoader\n",
    "from enc_dec.geo_webgnn_decoder import WeBGNNDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35a7de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change in original code: added clearing the arguments\n",
    "import sys\n",
    "sys.argv = [sys.argv[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2004912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def arg_parse():\n",
    "    parser = argparse.ArgumentParser(description='COEMBED ARGUMENTS.')\n",
    "    # ADD FOLLOWING ARGUMENTS\n",
    "    parser.add_argument('--cuda', dest = 'cuda',\n",
    "                help = 'CUDA.')\n",
    "    parser.add_argument('--parallel', dest = 'parallel',\n",
    "                help = 'Parrallel Computing')\n",
    "    parser.add_argument('--GPU IDs', dest = 'gpu_ids',\n",
    "                help = 'GPU IDs')\n",
    "    parser.add_argument('--add-self', dest = 'adj_self',\n",
    "                help = 'Graph convolution add nodes themselves.')\n",
    "    parser.add_argument('--model', dest = 'model',\n",
    "                help = 'Model load.')\n",
    "    parser.add_argument('--lr', dest = 'lr', type = float,\n",
    "                help = 'Learning rate.')\n",
    "    parser.add_argument('--batch-size', dest = 'batch_size', type = int,\n",
    "                help = 'Batch size.')\n",
    "    parser.add_argument('--num_workers', dest = 'num_workers', type = int,\n",
    "                help = 'Number of workers to load data.')\n",
    "    parser.add_argument('--epochs', dest = 'num_epochs', type = int,\n",
    "                help = 'Number of epochs to train.')\n",
    "    parser.add_argument('--input-dim', dest = 'input_dim', type = int,\n",
    "                help = 'Input feature dimension')\n",
    "    parser.add_argument('--hidden-dim', dest = 'hidden_dim', type = int,\n",
    "                help = 'Hidden dimension')\n",
    "    parser.add_argument('--output-dim', dest = 'output_dim', type = int,\n",
    "                help = 'Output dimension')\n",
    "    parser.add_argument('--num-gc-layers', dest = 'num_gc_layers', type = int,\n",
    "                help = 'Number of graph convolution layers before each pooling')\n",
    "    parser.add_argument('--dropout', dest = 'dropout', type = float,\n",
    "                help = 'Dropout rate.')\n",
    "\n",
    "    # SET DEFAULT INPUT ARGUMENT\n",
    "    parser.set_defaults(cuda = '0',\n",
    "                        parallel = False,\n",
    "                        add_self = '0', # 'add'\n",
    "                        adj = '0', # 'sym'\n",
    "                        model = '0', # 'load'\n",
    "                        kfold_num = 5,\n",
    "                        lr = 0.001,\n",
    "                        clip= 2.0,\n",
    "                        batch_size = 64,\n",
    "                        num_epochs = 200,\n",
    "                        num_workers = 0,\n",
    "                        input_dim = 4,\n",
    "                        hidden_dim = 4,\n",
    "                        output_dim = 36,\n",
    "                        decoder_dim = 150,\n",
    "                        num_classes = 1,\n",
    "                        num_gc_layer = 3,\n",
    "                        dropout = 0.01)\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c93db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate_schedule(args, dl_input_num, iteration_num, e1, e2, e3, e4):\n",
    "    epoch_iteration = int(dl_input_num / args.batch_size)\n",
    "    l1 = (args.lr - 0.0008) / (e1 * epoch_iteration)\n",
    "    l2 = (0.0008 - 0.0006) / (e2 * epoch_iteration)\n",
    "    l3 = (0.0006 - 0.0005) / (e3 * epoch_iteration)\n",
    "    l4 = (0.0005 - 0.0001) / (e4 * epoch_iteration)\n",
    "    l5 = 0.0001\n",
    "    if iteration_num <= (e1 * epoch_iteration):\n",
    "        learning_rate = args.lr - iteration_num * l1\n",
    "    elif iteration_num <= (e1 + e2) * epoch_iteration:\n",
    "        learning_rate = 0.0008 - (iteration_num - e1 * epoch_iteration) * l2\n",
    "    elif iteration_num <= (e1 + e2 + e3) * epoch_iteration:\n",
    "        learning_rate = 0.0006 - (iteration_num - (e1 + e2) * epoch_iteration) * l3\n",
    "    elif iteration_num <= (e1 + e2 + e3 + e4) * epoch_iteration:\n",
    "        learning_rate = 0.0005 - (iteration_num - (e1 + e2 + e3) * epoch_iteration) * l4\n",
    "    else:\n",
    "        learning_rate = l5\n",
    "    print('-------LEARNING RATE: ' + str(learning_rate) + '-------' )\n",
    "    return learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc7ee71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_geowebgnn_model(args, device, dataset):\n",
    "    print('--- BUILDING UP WEBGNN MODEL ... ---')\n",
    "    # GET PARAMETERS\n",
    "    # [num_gene, num_drug, (adj)node_num]\n",
    "    final_annotation_gene_df = pd.read_csv('./' + dataset + '/filtered_data/kegg_gene_annotation.csv')\n",
    "    gene_name_list = list(final_annotation_gene_df['kegg_gene'])\n",
    "    num_gene = len(gene_name_list)\n",
    "    drug_num_dict_df = pd.read_csv('./' + dataset + '/filtered_data/drug_num_dict.csv')\n",
    "    drug_dict = dict(zip(drug_num_dict_df.Drug, drug_num_dict_df.drug_num))\n",
    "    num_drug = len(drug_dict)\n",
    "    node_num = num_gene + num_drug\n",
    "    # [num_gene_edge, num_drug_edge]\n",
    "    gene_num_df = pd.read_csv('./' + dataset + '/filtered_data/kegg_gene_num_interaction.csv')\n",
    "    gene_num_df = gene_num_df.drop_duplicates()\n",
    "    drugbank_num_df = pd.read_csv('./' + dataset + '/filtered_data/final_drugbank_num_sym.csv')\n",
    "    num_gene_edge = gene_num_df.shape[0]\n",
    "    num_drug_edge = drugbank_num_df.shape[0]\n",
    "    num_edge = num_gene_edge + num_drug_edge\n",
    "    # import pdb; pdb.set_trace()\n",
    "    # BUILD UP MODEL\n",
    "    model = WeBGNNDecoder(input_dim=args.input_dim, hidden_dim=args.hidden_dim, embedding_dim=args.output_dim, \n",
    "                decoder_dim=args.decoder_dim, node_num=node_num, num_edge=num_edge, num_gene_edge=num_gene_edge, device=device)\n",
    "    model = model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd089851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d77059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_geowebgnn_model(dataset_loader, model, device, args, learning_rate):\n",
    "    optimizer = torch.optim.Adam(filter(lambda p : p.requires_grad, model.parameters()), lr=learning_rate, eps=1e-7, weight_decay=1e-6)\n",
    "    batch_loss = 0\n",
    "    for batch_idx, data in enumerate(dataset_loader):\n",
    "        optimizer.zero_grad()\n",
    "        x = Variable(data.x.float(), requires_grad=False).to(device)\n",
    "        edge_index = Variable(data.edge_index, requires_grad=False).to(device)\n",
    "        drug_index = Variable(data.drug_index, requires_grad=False).to(device)\n",
    "        label = Variable(data.label, requires_grad=False).to(device)\n",
    "        ypred = model(x, edge_index, drug_index)\n",
    "        loss = model.loss(ypred, label)\n",
    "        loss.backward()\n",
    "        batch_loss += loss.item()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "        optimizer.step()\n",
    "    torch.cuda.empty_cache()\n",
    "    return model, batch_loss, ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde27bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_geowebgnn(args, fold_n, load_path, iteration_num, device, dataset):\n",
    "    # TRAINING DATASET BASIC PARAMETERS\n",
    "    # [num_feature, num_gene, num_drug]\n",
    "    num_feature = 4\n",
    "    dict_drug_num = pd.read_csv('./' + dataset + '/filtered_data/drug_num_dict.csv')\n",
    "    num_drug = dict_drug_num.shape[0]\n",
    "    final_annotation_gene_df = pd.read_csv('./' + dataset + '/filtered_data/kegg_gene_annotation.csv')\n",
    "    num_gene = final_annotation_gene_df.shape[0]\n",
    "    form_data_path = './' + dataset + '/form_data'\n",
    "\n",
    "    import pdb; pdb.set_trace()\n",
    "\n",
    "    # READ THESE FEATURE LABEL FILES\n",
    "    print('--- LOADING TRAINING FILES ... ---')\n",
    "    xTr = np.load('./' + dataset + '/form_data/xTr' + str(fold_n) + '.npy')\n",
    "    yTr = np.load('./' + dataset + '/form_data/yTr' + str(fold_n) + '.npy')\n",
    "    drugTr =  np.load('./' + dataset + '/form_data/drugTr' + str(fold_n) + '.npy')\n",
    "    edge_index = torch.from_numpy(np.load(form_data_path + '/edge_index.npy')).long() \n",
    "\n",
    "\n",
    "    # BUILD [WeightBiGNN, DECODER] MODEL\n",
    "    model = build_geowebgnn_model(args, device, dataset)\n",
    "    if args.model == 'load':\n",
    "        model.load_state_dict(torch.load(load_path, map_location=device))\n",
    "\n",
    "    # TRAIN MODEL ON TRAINING DATASET\n",
    "    # OTHER PARAMETERS\n",
    "    dl_input_num = xTr.shape[0]\n",
    "    epoch_num = args.num_epochs\n",
    "    learning_rate = args.lr\n",
    "    batch_size = args.batch_size\n",
    "    # RECORD EPOCH LOSS AND PEARSON CORRELATION\n",
    "    if args.model != 'load':\n",
    "        iteration_num = 0\n",
    "    max_test_corr = 0\n",
    "    max_test_corr_id = 0\n",
    "    e1 = 10\n",
    "    e2 = 10\n",
    "    e3 = 10\n",
    "    e4 = 30\n",
    "    epoch_loss_list = []\n",
    "    epoch_pearson_list = []\n",
    "    test_loss_list = []\n",
    "    test_pearson_list = []\n",
    "    # CLEAN RESULT PREVIOUS EPOCH_I_PRED FILES\n",
    "    folder_name = 'epoch_' + str(epoch_num)\n",
    "    path = './' + dataset + '/result/%s' % (folder_name)\n",
    "    unit = 1\n",
    "    while os.path.exists('./' + dataset + '/result') == False:\n",
    "        os.mkdir('./' + dataset + '/result')\n",
    "    while os.path.exists(path):\n",
    "        path = './' + dataset + '/result/%s_%d' % (folder_name, unit)\n",
    "        unit += 1\n",
    "    os.mkdir(path)\n",
    "    # import pdb; pdb.set_trace()\n",
    "    for i in range(1, epoch_num + 1):\n",
    "        print('---------------------------EPOCH: ' + str(i) + ' ---------------------------')\n",
    "        print('---------------------------EPOCH: ' + str(i) + ' ---------------------------')\n",
    "        print('---------------------------EPOCH: ' + str(i) + ' ---------------------------')\n",
    "        print('---------------------------EPOCH: ' + str(i) + ' ---------------------------')\n",
    "        print('---------------------------EPOCH: ' + str(i) + ' ---------------------------')\n",
    "        model.train()\n",
    "        epoch_ypred = np.zeros((1, 1))\n",
    "        upper_index = 0\n",
    "        batch_loss_list = []\n",
    "        dl_input_num = xTr.shape[0]\n",
    "        for index in range(0, dl_input_num, batch_size):\n",
    "            if (index + batch_size) < dl_input_num:\n",
    "                upper_index = index + batch_size\n",
    "            else:\n",
    "                upper_index = dl_input_num\n",
    "            geo_datalist = read_batch(index, upper_index, xTr, yTr, drugTr,\\\n",
    "                num_feature, num_gene, num_drug, edge_index)\n",
    "            dataset_loader, node_num, feature_dim = GeoGraphLoader.load_graph(geo_datalist, prog_args)\n",
    "            # ACTIVATE LEARNING RATE SCHEDULE\n",
    "            iteration_num += 1\n",
    "            learning_rate = learning_rate_schedule(args, dl_input_num, iteration_num, e1, e2, e3, e4)\n",
    "            # learning_rate = 0.001\n",
    "            print('TRAINING MODEL...')\n",
    "            model, batch_loss, batch_ypred = train_geowebgnn_model(dataset_loader, model, device, args, learning_rate)\n",
    "            print('BATCH LOSS: ', batch_loss)\n",
    "            batch_loss_list.append(batch_loss)\n",
    "            # PRESERVE PREDICTION OF BATCH TRAINING DATA\n",
    "            batch_ypred = (Variable(batch_ypred).data).cpu().numpy()\n",
    "            epoch_ypred = np.vstack((epoch_ypred, batch_ypred))\n",
    "        epoch_loss = np.mean(batch_loss_list)\n",
    "        print('TRAIN EPOCH ' + str(i) + ' MSE LOSS: ', epoch_loss)\n",
    "        epoch_loss_list.append(epoch_loss)\n",
    "        epoch_ypred = np.delete(epoch_ypred, 0, axis = 0)\n",
    "        print(epoch_ypred)\n",
    "        print('ITERATION NUMBER UNTIL NOW: ' + str(iteration_num))\n",
    "        # PRESERVE PEARSON CORR FOR EVERY EPOCH\n",
    "        score_lists = list(yTr)\n",
    "        score_list = [item for elem in score_lists for item in elem]\n",
    "        epoch_ypred_lists = list(epoch_ypred)\n",
    "        epoch_ypred_list = [item for elem in epoch_ypred_lists for item in elem]\n",
    "        train_dict = {'Score': score_list, 'Pred Score': epoch_ypred_list}\n",
    "        tmp_training_input_df = pd.DataFrame(train_dict)\n",
    "        # pdb.set_trace()\n",
    "        epoch_pearson = tmp_training_input_df.corr(method='pearson')\n",
    "        epoch_pearson_list.append(epoch_pearson['Pred Score'][0])\n",
    "        tmp_training_input_df.to_csv(path + '/TrainingPred_' + str(i) + '.txt', index=False, header=True)\n",
    "        print('EPOCH ' + str(i) + ' PEARSON CORRELATION: ', epoch_pearson)\n",
    "        print('\\n-------------EPOCH TRAINING PEARSON CORRELATION LIST: -------------')\n",
    "        print(epoch_pearson_list)\n",
    "        print('\\n-------------EPOCH TRAINING MSE LOSS LIST: -------------')\n",
    "        print(epoch_loss_list)\n",
    "        epoch_pearson_array = np.array(epoch_pearson_list)\n",
    "        epoch_loss_array = np.array(epoch_loss_list)\n",
    "        np.save(path + '/pearson.npy', epoch_pearson_array)\n",
    "        np.save(path + '/loss.npy', epoch_loss_array)\n",
    "\n",
    "        # # # TEST MODEL ON TEST DATASET\n",
    "        # fold_n = 1\n",
    "        test_save_path = path\n",
    "        test_pearson, test_loss, tmp_test_input_df = test_geowebgnn(prog_args, fold_n, model, test_save_path, device, dataset)\n",
    "        test_pearson_score = test_pearson['Pred Score'][0]\n",
    "        test_pearson_list.append(test_pearson_score)\n",
    "        test_loss_list.append(test_loss)\n",
    "        tmp_test_input_df.to_csv(path + '/TestPred' + str(i) + '.txt', index = False, header = True)\n",
    "        print('\\n-------------EPOCH TEST PEARSON CORRELATION LIST: -------------')\n",
    "        print(test_pearson_list)\n",
    "        print('\\n-------------EPOCH TEST MSE LOSS LIST: -------------')\n",
    "        print(test_loss_list)\n",
    "        # SAVE BEST TEST MODEL\n",
    "        if test_pearson_score > max_test_corr:\n",
    "            max_test_corr = test_pearson_score\n",
    "            max_test_corr_id = i\n",
    "            # torch.save(model.state_dict(), path + '/best_train_model'+ str(i) +'.pt')\n",
    "            torch.save(model.state_dict(), path + '/best_train_model.pt')\n",
    "        print('\\n-------------BEST TEST PEARSON CORR MODEL ID INFO:' + str(max_test_corr_id) + '-------------')\n",
    "        print('--- TRAIN ---')\n",
    "        print('BEST MODEL TRAIN LOSS: ', epoch_loss_list[max_test_corr_id - 1])\n",
    "        print('BEST MODEL TRAIN PEARSON CORR: ', epoch_pearson_list[max_test_corr_id - 1])\n",
    "        print('--- TEST ---')\n",
    "        print('BEST MODEL TEST LOSS: ', test_loss_list[max_test_corr_id - 1])\n",
    "        print('BEST MODEL TEST PEARSON CORR: ', test_pearson_list[max_test_corr_id - 1])\n",
    "        torch.save(model.state_dict(), path + '/best_train_model.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1c84fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_geowebgnn_model(dataset_loader, model, device, args):\n",
    "    batch_loss = 0\n",
    "    for batch_idx, data in enumerate(dataset_loader):\n",
    "        x = Variable(data.x, requires_grad=False).to(device)\n",
    "        edge_index = Variable(data.edge_index, requires_grad=False).to(device)\n",
    "        drug_index = Variable(data.drug_index, requires_grad=False).to(device)\n",
    "        label = Variable(data.label, requires_grad=True).to(device)\n",
    "        # THIS WILL USE METHOD [def forward()] TO MAKE PREDICTION\n",
    "        ypred = model(x, edge_index, drug_index)\n",
    "        loss = model.loss(ypred, label)\n",
    "        batch_loss += loss.item()\n",
    "    torch.cuda.empty_cache()\n",
    "    return model, batch_loss, ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f13fa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_geowebgnn(args, fold_n, model, test_save_path, device, dataset):\n",
    "    print('-------------------------- TEST START --------------------------')\n",
    "    print('-------------------------- TEST START --------------------------')\n",
    "    print('-------------------------- TEST START --------------------------')\n",
    "    print('-------------------------- TEST START --------------------------')\n",
    "    print('-------------------------- TEST START --------------------------')\n",
    "    # TEST MODEL ON TEST DATASET\n",
    "    form_data_path = './' + dataset + '/form_data'\n",
    "    xTe = np.load(form_data_path + '/xTe' + str(fold_n) + '.npy')\n",
    "    yTe = np.load(form_data_path + '/yTe' + str(fold_n) + '.npy')\n",
    "    drugTe =  np.load('./' + dataset + '/form_data/drugTe' + str(fold_n) + '.npy')\n",
    "    edge_index = torch.from_numpy(np.load(form_data_path + '/edge_index.npy') ).long() \n",
    "\n",
    "    dl_input_num = xTe.shape[0]\n",
    "    batch_size = args.batch_size\n",
    "    # CLEAN RESULT PREVIOUS EPOCH_I_PRED FILES\n",
    "    path = test_save_path\n",
    "    # [num_feature, num_gene, num_drug]\n",
    "    num_feature = 4\n",
    "    dict_drug_num = pd.read_csv('./' + dataset + '/filtered_data/drug_num_dict.csv')\n",
    "    num_drug = dict_drug_num.shape[0]\n",
    "    final_annotation_gene_df = pd.read_csv('./' + dataset + '/filtered_data/kegg_gene_annotation.csv')\n",
    "    num_gene = final_annotation_gene_df.shape[0]\n",
    "    # RUN TEST MODEL\n",
    "    model.eval()\n",
    "    all_ypred = np.zeros((1, 1))\n",
    "    upper_index = 0\n",
    "    batch_loss_list = []\n",
    "    for index in range(0, dl_input_num, batch_size):\n",
    "        if (index + batch_size) < dl_input_num:\n",
    "            upper_index = index + batch_size\n",
    "        else:\n",
    "            upper_index = dl_input_num\n",
    "        geo_datalist = read_batch(index, upper_index, xTe, yTe, drugTe,\\\n",
    "                num_feature, num_gene, num_drug, edge_index)\n",
    "        dataset_loader, node_num, feature_dim = GeoGraphLoader.load_graph(geo_datalist, prog_args)\n",
    "        print('TEST MODEL...')\n",
    "        # import pdb; pdb.set_trace()\n",
    "        model, batch_loss, batch_ypred = test_geowebgnn_model(dataset_loader, model, device, args)\n",
    "        print('BATCH LOSS: ', batch_loss)\n",
    "        batch_loss_list.append(batch_loss)\n",
    "        # PRESERVE PREDICTION OF BATCH TEST DATA\n",
    "        batch_ypred = (Variable(batch_ypred).data).cpu().numpy()\n",
    "        all_ypred = np.vstack((all_ypred, batch_ypred))\n",
    "    test_loss = np.mean(batch_loss_list)\n",
    "    print('MSE LOSS: ', test_loss)\n",
    "    # PRESERVE PEARSON CORR FOR EVERY EPOCH\n",
    "    all_ypred = np.delete(all_ypred, 0, axis = 0)\n",
    "    all_ypred_lists = list(all_ypred)\n",
    "    all_ypred_list = [item for elem in all_ypred_lists for item in elem]\n",
    "    score_lists = list(yTe)\n",
    "    score_list = [item for elem in score_lists for item in elem]\n",
    "    test_dict = {'Score': score_list, 'Pred Score': all_ypred_list}\n",
    "    tmp_test_input_df = pd.DataFrame(test_dict)\n",
    "    test_pearson = tmp_test_input_df.corr(method = 'pearson')\n",
    "    print('PEARSON CORRELATION: ', test_pearson)\n",
    "    print('FOLD - ', fold_n)\n",
    "    return test_pearson, test_loss, tmp_test_input_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edb4dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# PARSE ARGUMENT FROM TERMINAL OR DEFAULT PARAMETERS\n",
    "prog_args = arg_parse()\n",
    "\n",
    "# CHECK AND ALLOCATE RESOURCES\n",
    "device, prog_args.gpu_ids = utils.get_available_devices()\n",
    "# MANUAL SET\n",
    "\n",
    "# Change in original code: Set device to cpu if cuda is not available\n",
    "\n",
    "# device = torch.device('cuda:0') \n",
    "# torch.cuda.set_device(device)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    torch.cuda.set_device(device)\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "\n",
    "print('MAIN DEVICE: ', device)\n",
    "# SINGLE GPU\n",
    "\n",
    "# Change in original code: Set device to cpu if cuda is not available\n",
    "\n",
    "# prog_args.gpu_ids = [0]\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    prog_args.gpu_ids = [0]\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "else:\n",
    "    prog_args.gpu_ids = []\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "# # TRAIN THE MODEL\n",
    "# TRAIN [FOLD-1]\n",
    "fold_n = 2\n",
    "# dataset = 'data-nci'\n",
    "# dataset = 'data-oneil'\n",
    "dataset = 'data-drugcomb-fi'\n",
    "# dataset = 'data-DrugCombDB'\n",
    "load_path = ''\n",
    "yTr = np.load('./' + dataset + '/form_data/yTr' + str(fold_n) + '.npy')\n",
    "# yTr = np.load('./' + dataset + '/form_data/y_split1.npy')\n",
    "dl_input_num = yTr.shape[0]\n",
    "epoch_iteration = int(dl_input_num / prog_args.batch_size)\n",
    "start_iter_num = 100 * epoch_iteration\n",
    "train_geowebgnn(prog_args, fold_n, load_path, start_iter_num, device, dataset)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
